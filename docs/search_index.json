[
["index.html", "Using AWS EC2 Compute and S3 Storage for Model Fitting Introduction Table of Contents Authors and Sources", " Using AWS EC2 Compute and S3 Storage for Model Fitting August 2019 Introduction Table of Contents In this tutorial, we first provide an overview of AWS Elastic Compute Cloud (EC2) and Simple Storage Service (S3) services. As an example of using them for model fitting, we then will launch an Amazon Deep Learning Instance on EC2 and run a Multi-Layer Perceptron model that predicts the wireless network intrusion activities from a customized dataset uploaded and stored in an AWS S3 bucket. We will do these tasks using AWS Management Console. This tutorial is organized into the following parts: 1. Introduce AWS EC2 and S3 Services 2. Set Up an AWS Account 3. Launch an Amazon Deep Learning Amazon Machine Images (AMI) Instance 4. Grant an AMI instance access to an S3 bucket 5. Use the S3 Console to Store and Transfer Your Data 6. Retrieve the IP address of your Deep Learning AMI instance and connect to the instance 7. Launch a Jupyter notebook 8. Create, train, and evaluate a Multi-Layer Perceptron model using neural networks 9. Finally, get the results locally This tutorial is still under development, and we take feedback! If you find anything confusing or think the guide misses important content, please email help@iq.harvard.edu. Authors and Sources Steve Worthington and Jinjie Liu at the IQSS designed the structure of the guide and wrote down the content. This tutorial was written based on the Lab Guide of the Deep Learning on AWS 1.3 Training Course. We added to it with three new contents: (1) use the S3 Console to store and transfer your data; (2) allow an EC2 instance to read data from and write computational results to an S3 bucket; and (3) use TensorFlow programming framework for developing a deep learning model. We also referenced Bernard Golden’s book Amazon Web Services for Dummies for the conceptual issues and the online AWS documentation for implementation. "],
["aws-ec2-and-s3-services.html", "1 AWS EC2 and S3 Services 1.1 EC2 Services 1.2 S3 Services", " 1 AWS EC2 and S3 Services Amazon Web Services (AWS) is a public cloud computing provider. Unlike enterprise cloud providers, AWS allows anyone with an e-mail address and a credit card access to a large variety of services easily and quickly. Its software infrastructure is based on virtualization, operated to offer its computing capability as a service, and designed for high flexibility and reliability. This cheat sheet describes the family of AWS services and provides guidelines for using them. If you are interested, check out this link for additional online content dealing with AWS. Amazon Web Services continues to evolve rapidly. Look there to learn the latest about AWS. The most popular services are EC2 and S3. EC2 is the AWS computing service, which offers computing capacity on demand with immediate availability and no set commitment to length of use. S3 is the AWS’s first service. It offers the object storage over the web. We will explain these two services in some detail in the following sections. 1.1 EC2 Services In earlier days when you needed a server, you had to buy one, and then have the server delivered, installed, and connected to the network. Finally, you gained access to your server. It wasn’t uncommon for this process to take from three to six months. EC2 provides virtual servers in a matter of minutes, all via self-service. It first has a virtualization layer that uses virtual machines to provide a virtual server. It then overlaid its virtualization layer with a sophisticated software layer designed to obviate the need for human intervention in the provisioning process of a virtual machine. With this innovation from Amazon, a fundamental part of the entire IT industry – the use of provisioning servers – has been shifted. EC2 is based on virtualization — the process of using software to create virtual machines that then carry out all the tasks you’d associate with a “real” computer using a “real” operating system. EC2 has come up with its own terminology: When a virtual machine is running in EC2, it’s referred to as an instance; when an instance isn’t running in EC2, it’s referred to as an image. Likewise, in virtualization, a virtual machine is started, and in EC2 an instance is launched. An Amazon Machine Image (AMI) is the collection of bits needed to create a running instance. This collection includes the three essential elements: (1) at minimum, the operating system that will run on the instance, (2) any software packages you’ve chosen to install, and (3) any configuration information needed for the instance to operate properly. You choose which AMIs to use for your application based on these elements. You also need to consider instance types – the types of virtual machines you can run in AWS. Instances vary by the amount of three types of compute resources: (1) processing power in terms of a certain number of EC2 compute units (ECU), (2) amount of memory, measured in gigabytes, and (3) amount of disk storage. Every instance also comes supplied with one virtual network interface card (NIC), which it uses to communicate with other devices or services. Every instance is given two IP addresses: one private address that’s used solely within AWS and one public address that’s used for internet access to the instance. Depending on your application’s operating characteristics, you can choose which instance types to use (compute optimized, memory optimized, or storage optimized, for example). After you choose an instance type, you also have to choose an image size that is suitable for your application. The variety of image sizes provides a range of computing resources available for an instance type. If you find all these mindboggling, there is an excellent third-party website that lists and compares all the different instance types and sizes. 1.2 S3 Services The enormous growth of storage makes traditional approaches (local storage, network-attached storage, storage-area networks, and the like) no longer appropriate, for extremely large amount of data, for fast transfer of those data, and for the affordability at such scale. S3 provides highly scalable object storage in the form of unstructured collections of bits. Individual users tend to use S3 as secure, location-independent storage of digital media. Another common personal use for S3 is to back up local files. S3 objects are treated as web objects — that is, they’re accessed via Internet protocols using a URL identifier, in this format: http://s3.amazonaws.com/bucket/key. A bucket in AWS is a group of objects. A key in AWS is the name of an object, and it acts as an identifier to locate the data associated with the key. This convenient arrangement provides a familiar directory-like or URL-like format for object names; however, it doesn’t represent the actual structure of the S3 storage system. In the following sections, we provide an example of how to use EC2 and S3 services for model fitting. In this example, we will launch an Amazon Deep Learning Instance on EC2 and run a Multi-Layer Perceptron model that predicts the wireless network intrusion activities based on a customized dataset. We upload and store this dataset in an S3 bucket. We show the procedures that allow an EC2 instance to read the data from and write computational results to the bucket. "],
["set-up-an-aws-account.html", "2 Set Up an AWS Account", " 2 Set Up an AWS Account The first thing to do is to create your very own AWS account. Amazon updates the Management Console screens frequently, so the instructions may be different than what you see displayed on your terminal. Point your favorite web browser to the main Amazon Web Services page at http://aws.amazon.com. Choose Create an AWS Account. Enter a username, your e-mail address, and the password you want to use. Then choose Continue. Choose Personal or Professional. Note: Personal accounts and professional accounts have the same features and functions. On the Account Information screen, enter your address and phone number information. You’re asked to select the box confirming that you agree to the terms of the AWS customer agreement. Enter the required personal information, confirm your acceptance of the customer agreement, and then click the Create Account and Continue button. Note: You receive an email to confirm that your account is created. You can sign in to your new account using the email address and password you supplied. However, you can’t use AWS services until you finish activating your account. Enter the required payment information in the appropriate fields and then choose Secure Submit. Verify your phone number Choose whether you want to verify your account by Text message (SMS) or a Voice call. Choose your country or region code from the list. Enter a phone number where you can be reached in the next few minutes. Enter the code displayed in the captcha. When you’re ready, choose Contact me. In a few moments, an automated system will contact you. Note: If you chose to verify your account by SMS, choose Send SMS instead. Enter the PIN you receive by text message or voice call, and then choose Continue. On the Select a Support Plan page, choose one of the available Support plans. For a description of the available Support plans and their benefits, see Compare AWS Support Plans. After you choose a Support plan, a confirmation page indicates that your account is being activated. Accounts are usually activated within a few minutes, but the process might take up to 24 hours. You can sign in to your AWS account during this time. The AWS home page might display a button that shows “Complete Sign Up” during this time, even if you’ve completed all the steps in the sign-up process. When your account is fully activated, you’ll receive a confirmation email. After you receive this email, you have full access to all AWS services. "],
["launch-an-amazon-deep-learning-amazon-machine-images-ami-instance.html", "3 Launch an Amazon Deep Learning Amazon Machine Images (AMI) instance", " 3 Launch an Amazon Deep Learning Amazon Machine Images (AMI) instance Log in your AWS account. This will go to the AWS Management Console. In the AWS Management Console, click on the Services menu, then click EC2. On the upper right corner, click the region button, select US East (N. Virginia). Click Launch Instance. On the Step 1: Choose an Amazon Machine Image (AMI) page, scroll down to Deep Learning AMI (Amazon Linux). Confirm that it is the Amazon Linux version, not the Ubuntu or Windows version. There are multiple versions, so select the highest version. For Deep Learning AMI (Amazon Linux), click Select. The Deep Learning AMI is designed to provide a stable, secure, and high-performance execution environment for deep learning applications running on Amazon EC2. It includes popular deep learning frameworks, including MXNet, TensorFlow, PyTorch, Keras, Chainer, Caffe, Theano, and CNTK. On Step 2: Choose an Instance type page, for Filter by, select GPU instances. Select p2.xlarge. Click Next: Configure Instance Details. On the Step 3: Configure Instance Details page, keep the default settings. Click Next: Add Storage. On the Step 4: Add Storage, keep the default storage settings. Click Next: Add Tags. On the Step 5: Add Tags page, click Add Tag, then: For Key, type: Name For Value, type: Deep Learning Development Click Review and Launch. On the Step 7: Review Instance Launch page, review your instance settings and click Launch. On Select an existing key pair or create a new key pair window, if applicable, select Create a new key pair from the dropdown menu. Assign a key pair name (for example, DeepL) and download the private key file in a secure and accessible location. Amazon EC2 gives you a PEM file. So, you would need to convert it to PPK files later for Windows SSH connectivity to the instance. Click Launch Instances. The launch could fail if you choose regions other than US East (N. Virginia) at the first beginning. You could either change the region to US East (N. Virginia) and retry or launch it after you get an email saying that your account is now fully verified in that region. This verification will take several minutes. "],
["grant-an-ec2-instance-access-to-s3-bucket.html", "4 Grant an EC2 Instance Access to S3 Bucket 4.1 Create an Identity and Access Management (IAM) Role 4.2 Attach an IAM Role to an Instance", " 4 Grant an EC2 Instance Access to S3 Bucket 4.1 Create an Identity and Access Management (IAM) Role Open the IAM console at https://console.aws.amazon.com/iam/. In the navigation pane, choose Roles, Create role. On the Select role type page, choose EC2 and the EC2 use case. Choose Next: Permissions. On the Attach permissions policy page, select an AWS managed policy that grants your instances access to the resources that they need. In this case, choose full access to all buckets via management console. On the Review page, type a name for the role (for example, s3access) and choose Create role. 4.2 Attach an IAM Role to an Instance To attach an IAM role to an instance that has no role, the instance can be in the stopped or running state. 36. Open the Amazon EC2 console. 37. In the navigation pane, choose Instances. 38. Select the instance, choose Actions, Instance Settings, Attach/Replace IAM role. 39. Select the IAM role to attach to your instance and choose Apply. "],
["use-the-amazon-simple-storage-service-s3-console-to-store-and-transfer-your-data.html", "5 Use the Amazon Simple Storage Service (S3) Console to Store and Transfer Your Data 5.1 Create a Bucket 5.2 Upload Your Dataset", " 5 Use the Amazon Simple Storage Service (S3) Console to Store and Transfer Your Data 5.1 Create a Bucket In the AWS Management Console, click Services, then click S3 to open the console. Choose Create bucket. On the Name and region page, type a name for your bucket (awid, for example) and choose the AWS Region (US East (N. Virginia), for example) where you want the bucket to reside. Choose a Region close to you. (Optional) If you have already set up a bucket that has the same settings that you want to use for the new bucket that you want to create, you can set it up quickly by choosing Copy settings from an existing bucket, and then choosing the bucket whose settings you want to copy. The settings for the following bucket properties are copied: versioning, tags, and logging. Do one of the following: If you copied settings from another bucket, choose Create. You’re done, so skip the following steps. If not, choose Next. On the Configure options page, for Versioning, select Keep all versions of an object in the same bucket. to enable object versioning for the bucket. For Server access logging, select Log requests for access to your bucket. to enable server access logging on the bucket. Server access logging provides detailed records for the requests that are made to your bucket. For Tags, you can use cost allocation bucket tags to annotate billing for your use of a bucket. Each tag is a key-value pair that represents a label that you assign to a bucket. To add a tag, enter a Key and a Value. Choose Add another to add another tag. For Object-level logging, select Record object-level API activity by using CloudTrail for an additional cost to enable object-level logging with CloudTrail. For Default encryption, select Automatically encrypt objects when they are stored in S3 to enable default encryption for the bucket. For Object lock, select Permanently allow objects in this bucket to be locked if you want to be able to lock objects in the bucket. Object lock requires that you enable versioning on the bucket. For CloudWatch request metrics, select Monitor requests in your bucket for an additional cost. to configure CloudWatch request metrics for the bucket. Choose Next. On the Set permissions page, under Block public access (bucket settings), we recommend that you do not change the default settings that are listed under Block all public access. If you intend to use the bucket to store Amazon S3 server access logs, in the Manage system permissions list, choose Grant Amazon S3 Log Delivery group write access to this bucket. Choose Next. On the Review page, verify the settings. If you want to change something, choose Edit. If your current settings are correct, choose Create bucket. 5.2 Upload Your Dataset You can upload files or folders to S3 bucket by dragging and dropping. Drag and drop functionality is supported only for the Chrome and Firefox browsers. 57. In the Bucket name list, choose the name of the bucket that you want to upload your folders or files to. 58. In a window other than the console window, select the files and folders that you want to upload. Then drag and drop your selections into the console window that lists the objects in the destination bucket. The files you chose are listed in the Upload dialog box. 59. To immediately upload the listed files and folders without granting or removing permissions for specific users or setting public permissions for all the files that you’re uploading, choose Upload. "],
["retrieve-the-ip-address-of-your-deep-learning-ami-instance-and-connect-to-your-deep-learning-ami-instance.html", "6 Retrieve the IP Address of Your Deep Learning AMI Instance and Connect to Your Deep Learning AMI Instance 6.1 Connect to Your Deep Learning AMI Instance from a Windows Machine 6.2 Connect to Your Deep Learning AMI Instance from a Mac or Linux Machine", " 6 Retrieve the IP Address of Your Deep Learning AMI Instance and Connect to Your Deep Learning AMI Instance Open the Amazon EC2 console, click Instances. On the Instances page, click the Deep Learning Development instance. In the bottom pane, under Description tab, copy the IPv4 Public IP to your clipboard and paste the value into a text editor for later use. 6.1 Connect to Your Deep Learning AMI Instance from a Windows Machine You will set up an SSH tunnel in your PuTTY client in Windows. For security reasons, the Jupyter notebook is only available on the instance’s local webserver http://localhost:port and is not published on the Internet. To connect to the local webserver on the instance, you can create an SSH tunnel between your computer and the instance (this is also known as port forwarding). If you do not have PuTTY installed on your computer, download it from online. 63. Open PuTTYgen. 64. Click Load. Open DeepL.pem file. 65. Click Save private key. It is fine to save it without a passphrase to protect it. Save it to a safe and accessible location with name DeepL.ppk, for example. 66. Launch PuTTY by running the putty.exe file you downloaded. 67. For Host Name (or IP address), type ec2-user@ and then paste the IP address of the instance that you copied earlier. It should look like: ec2-user@11.22.33.44. 68. In the navigation panel, expand SSH and click Auth. 69. In the Private key file for authentication box, browse to the .ppk file that you downloaded earlier, then click Open. 70. In the navigation panel, click Tunnels. First, you will forward port 8888 to the remote port 8888 for Jupyter Notebooks: 71. For Source port, type: 8888 72. For Destination, type: localhost:8888 73. Click Add. The configuration should look like this: Click Open to start the session. If prompted to cache the server’s host key, click Yes. If the connection is not successful, the instance might still be launching. Wait two minutes then try connecting again by clicking the PuTTY icon in the top-left corner of the PuTTY window and selecting Restart session. If you are prompted for a username, type: ec2-usera. 6.2 Connect to Your Deep Learning AMI Instance from a Mac or Linux Machine Start a new Terminal session on your computer. Run the following command, replacing PATH-TO-PEM.pem with your *.PEM key path: chmod 400 PATH-TO-PEM.pem Example: chmod 400 /keys/qwiklabs1234.pem Run the following command, replacing the PATH-TO-PEM.pem with your *.PEM key and replacing YOUR-IP-ADDRESS with the instance IP address you copied earlier: ssh -i PATH-TO-PEM.pem -L 8888:localhost:8888 ec2-user@YOUR-IP-ADDRESS For example: ssh -i /keys/qwikLABS-L1234-12345.pem -L 8888:localhost:8888 ec2-user@11.22.33.44 If prompted to continue connecting, type: yes. "],
["explore-amazon-deep-learning-ami-and-connect-to-a-jupyter-notebook.html", "7 Explore Amazon Deep Learning AMI and Connect to a Jupyter Notebook", " 7 Explore Amazon Deep Learning AMI and Connect to a Jupyter Notebook Jupyter notebooks are documents that allow you to create and share documents that contain both code as well as rich text elements such as equations. If you are unfamiliar with Jupyter notebooks, see Jupyter notebook docs online. 80. In your SSH session, you will find various deep learning frameworks like MXNet, TensorFlow, Keras as well as Anaconda installed on the deep learning AMI. 81. Type source activate tensorflow_p36 (you could choose the framework you want to activate) in your SSH session. 82. Type nvidia-smi -L to check the availability of GPU on the instance. Your EC2 instance has a Nvidia GPU processor. 83. To start a Jupyter notebook, type nohup jupyter notebook --no-browser &amp; Press Enter if the command-line hangs while running. 84. To view the output of the previous command, type tail nohup.out. You should see a message like this: Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://localhost:8888/?token=xxxx. If you did not see this output, wait 30 seconds and run the command again until it appears. 85. Copy the URL from the terminal and paste it into a new web browser tab. 86. You should see the following page (or similar page): It might take a minute for the notebook page to appear. Give it time to fully load. 87. To create a Jupyter notebook, click New, then click Python3. A new tab will open that will display a Jupyter Notebook. You will next create a deep learning model in this notebook. "],
["create-and-run-a-multi-layer-perceptron-model-using-neural-networks.html", "8 Create and Run a Multi-Layer Perceptron Model Using Neural Networks 8.1 Install and Import Dependencies 8.2 Provide Utilities-Related Functions 8.3 Read Data from S3 and Inspect the Data 8.4 Perform the Data Processing Specific to Neural Networks 8.5 Create a Multi-Layer Perceptron Model Using Neural Networks 8.6 Train the Multi-Layer Perceptron Model 8.7 Evaluate the Multi-Layer Perceptron Model", " 8 Create and Run a Multi-Layer Perceptron Model Using Neural Networks The example below uses some basic functions from TensorFlow to create neural networks over the customized data sets stored in S3 and write the result plots to S3. The example uses AWID wireless network intrusion detection dataset with 154 features and a class label having 4 network activity type categories. The data set has been pre-processed with data transformation, normalization, and under-sampling. 8.1 Install and Import Dependencies Paste the following code into the In cell in the jupyter notebook to import dependencies into the notebook cell: import pandas as pd import numpy as np import tensorflow as tf import datetime import matplotlib.pyplot as plt from sklearn.utils import shuffle from smart_open import open smart_open is a Python 2 &amp; Python 3 library for efficient streaming of very large files from/to storages such as S3, HDFS, WebHDFS, HTTP, HTTPS, SFTP, or local filesystem. It builds on boto3 and other remote storage libraries but offers a clean unified Pythonic API. Unlike the other packages in the example, you need to install this package to the “tensorflow_p36” environment before you can import it in the code. To do that, go to jupyter Home tab, click Conda. Remember we have activated the “tensorflow_p36” environment. Select the “smart_open” package in the left bottom plane and move it to the right bottom plane by clicking the arrow button. You should see the “smart_open” package show up in the right bottom plane. Click Refresh package list. Repeat the steps 80-87 to make sure that the installed packages in the “tensorflow_p36” environment are fully updated. Run the cell by pressing Shift+Enter or click on Run. When the cell finishes running, the number on the left of the cell will change from In[*]: to In[1]. 8.2 Provide Utilities-Related Functions Provide the utilities functions for reading files, checking column names, checking missing values, and transforming the class labels from a categorical variable to 4 dummy variables. # Utilities-related functions def now(): tmp = datetime.datetime.now().strftime(&quot;%Y-%m-%d %H:%M:%S&quot;) return tmp def read_file(file): try: df = pd.read_csv(file, index_col = 0) print(&quot;{}: {} has {} observations and {} columns&quot;.format(now(), file, df.shape[0], df.shape[1])) print(&quot;{}: Column name checking::: {}&quot;.format(now(), df.columns.tolist())) except MemoryError as e: print(e.message) return df # Read data frame, check missing data, find number of missing def check_missing(df): try: if(isinstance(df, pd.DataFrame)): na_pool = pd.concat([df.isnull().any(), df.isnull().sum(), df.isnull().sum() / df.shape[0]], axis = 1, keys = [&quot;na_bool&quot;, &quot;na_sum&quot;, &quot;na_percent&quot;]) na_pool = na_pool.loc[na_pool[&quot;na_bool&quot;] == True] return na_pool else: print(&quot;{}: The input is not panda DataFrame&quot;.format(now())) except (UnboundLocalError, RuntimeError): print(&quot;{}: The input has something wrong&quot;.format(now())) def transform(df): df.loc[df.type == 1, &#39;isNormal&#39;] = 1 df.loc[df.type != 1, &#39;isNormal&#39;] = 0 df.loc[df.type == 2, &#39;isImpersonation&#39;] = 1 df.loc[df.type != 2, &#39;isImpersonation&#39;] = 0 df.loc[df.type == 3, &#39;isFlooding&#39;] = 1 df.loc[df.type != 3, &#39;isFlooding&#39;] = 0 df.loc[df.type == 4, &#39;isInjection&#39;] = 1 df.loc[df.type != 4, &#39;isInjection&#39;] = 0 print(df.isNormal.value_counts()) print(df.isImpersonation.value_counts()) print(df.isFlooding.value_counts()) print(df.isInjection.value_counts()) df2 = pd.concat([df.isNormal, df.isImpersonation, df.isFlooding, df.isInjection], axis = 1) df1 = df.drop([&#39;type&#39;, &#39;isNormal&#39;, &#39;isImpersonation&#39;, &#39;isFlooding&#39;, &#39;isInjection&#39;], axis = 1) return df1, df2 8.3 Read Data from S3 and Inspect the Data You need to find the Access Key and the Secret Access Key. Click on your username at the top right of the AWS Management Console page. Click on the My Security Credentials link from the drop-down menu. Find the Access keys (access key ID and secret access key) section and click on Create New Access Key. Download the Access Key file (in .csv format) to a safe local folder for later use. Paste the following code into the In cell in the jupyter notebook to read data (both training set and testing set) from S3 and then inspect the data: # Read in datafile aws_key = &#39;YOUR_AWS_ACCESS_KEY&#39; aws_secret = &#39;YOUR_AWS_SECRET_ACCESS_KEY&#39; bucket_name = &#39;YOUR_S3_BUCKET_NAME&#39; object_train_key = &#39;train.csv&#39; object_test_key = &#39;test.csv&#39; path_train = &#39;s3://{}:{}@{}/{}&#39;.format(aws_key, aws_secret, bucket_name, object_train_key) path_test = &#39;s3://{}:{}@{}/{}&#39;.format(aws_key, aws_secret, bucket_name, object_test_key) data_trn = read_file(smart_open(path_train)) print(check_missing(data_trn)) data_trn.rename(columns={&#39;154&#39;:&#39;type&#39;}, inplace=True) print(data_trn.head(5)) print(data_trn.type.value_counts()) data_tst = read_file(smart_open(path_test)) print(check_missing(data_tst)) data_tst.rename(columns={&#39;154&#39;:&#39;type&#39;}, inplace=True) print(data_tst.head(5)) print(data_tst.type.value_counts()) 8.4 Perform the Data Processing Specific to Neural Networks Split the training set into a training part and a validation part: normal = data_trn[data_trn.type == 1] impersonation = data_trn[data_trn.type == 2] flooding = data_trn[data_trn.type == 3] injection = data_trn[data_trn.type == 4] normal_trn = normal.sample(frac = 0.7) impersonation_trn = impersonation.sample(frac = 0.7) flooding_trn = flooding.sample(frac = 0.7) injection_trn = injection.sample(frac = 0.7) train = pd.concat([normal_trn, impersonation_trn, flooding_trn, injection_trn], axis = 0) validation = data_trn.loc[~data_trn.index.isin(train.index)] #shuffle the dataframes so that rows are in random order train = shuffle(train) validation = shuffle(validation) Transform the class labels from a categorical variable to 4 dummy variables and check the correctness: x_train, y_train = transform(train) x_validation, y_validation = transform(validation) x_test, y_test = transform(data_tst) # check to ensure that all of training and testing sets have correct shape print(x_train.shape) print(y_train.shape) print(x_validation.shape) print(y_validation.shape) print(x_test.shape) print(y_test.shape) 8.5 Create a Multi-Layer Perceptron Model Using Neural Networks Define parameters for your neural network: # parameters learning_rate = 0.005 data_size = x_train.shape[0] batch_size = 1150 training_epochs = 2000 training_dropout = 0.9 Define the neural network configuration: # input place holders x = tf.compat.v1.placeholder(tf.float32, [None, x_train.shape[1]]) y = tf.compat.v1.placeholder(tf.float32, [None, y_train.shape[1]]) rate = tf.compat.v1.placeholder(tf.float32) #weights, bias, and activation function for n layers num_nodes = int(x_train.shape[1] * (2 / 3)) w1 = tf.Variable(tf.random.normal([x_train.shape[1], num_nodes], stddev = 0.15)) b1 = tf.Variable(tf.random.normal([num_nodes])) a1 = tf.nn.relu(tf.matmul(x, w1) + b1) a1_out = tf.nn.dropout(a1, rate = rate) w2 = tf.Variable(tf.random.normal([num_nodes, y_train.shape[1]], stddev = 0.15)) b2 = tf.Variable(tf.random.normal([y_train.shape[1]])) z2 = tf.matmul(a1_out, w2) + b2 a2 = tf.nn.softmax(z2) If you define a multi-layer network with linear neurons, then the network will only be a linear function. To allow the network to capture non-linear properties, you can add an activation function at each layer. The activation functions could be ReLU, sigmoid, or tanh. 104. Define a cost function and an optimizer to learn the weights and biases: cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = z2, labels = y)) optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost) Define the evaluation metric: pred = tf.argmax(a2, 1) correct = tf.equal(tf.argmax(a2, 1), tf.argmax(y, 1)) accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) 8.6 Train the Multi-Layer Perceptron Model Set up the data structures and file paths to save the intermediate training results: # train model train_accuracy_summary = [] train_cost_summary = [] valid_accuracy_summary = [] valid_cost_summary = [] stop_early = 0 checkpoint1 = &#39;./best_model_pca.ckpt&#39; checkpoint2 = &#39;./weights_pca.ckpt&#39; saver = tf.compat.v1.train.Saver(max_to_keep = 1) weights_saver = tf.compat.v1.train.Saver(var_list = [w1]) We use the temporary storage associated with this EC2 Deep Learning instance to save the intermediate training results. Those results will be gone when the instance is terminated. It is not efficient to save those intermediate results to S3. Since the instance is run on Amazon Linux system (instead of Windows), please make sure to write down the correct file paths. 107. Run the training loop with the defined parameters in step 102. We save the model and its associated weights only for the model with the highest validation accuracy. We also use an early stop criterion to stop the training loop early if the model has been trained long enough and the validation accuracy cannot be improved further in the next 20 loops. with tf.compat.v1.Session() as sess: sess.run(tf.compat.v1.global_variables_initializer()) for epoch in range(training_epochs): for batch in range(int(data_size/batch_size)): batch_x = x_train[batch*batch_size: (1+batch)*batch_size] batch_y = y_train[batch*batch_size: (1+batch)*batch_size] sess.run([optimizer], feed_dict = {x: batch_x, y: batch_y, rate: 1 - training_dropout}) train_accuracy, train_cost = sess.run([accuracy, cost], feed_dict = {x: x_train, y: y_train, rate: 1 - training_dropout}) valid_accuracy, valid_cost = sess.run([accuracy, cost], feed_dict = {x: x_validation, y: y_validation, rate: 1 - training_dropout}) print(&quot;Epoch:&quot;, epoch, &quot;Train_Accuracy =&quot;, &quot;{:.5f}&quot;.format(train_accuracy), &quot;Train_Cost =&quot;, &quot;{:.5f}&quot;.format(train_cost), &quot;Valid_Accuracy =&quot;, &quot;{:.5f}&quot;.format(valid_accuracy), &quot;Valid_Cost =&quot;, &quot;{:.5f}&quot;.format(valid_cost)) if epoch &gt; 0 and valid_accuracy &gt; max(valid_accuracy_summary): saver.save(sess, checkpoint1) weights_saver.save(sess, checkpoint2) train_accuracy_summary.append(train_accuracy) train_cost_summary.append(train_cost) valid_accuracy_summary.append(valid_accuracy) valid_cost_summary.append(valid_cost) if valid_accuracy &lt; max(valid_accuracy_summary) and epoch &gt; 1000: stop_early += 1 if stop_early == 20: break else: stop_early = 0 print() print(&quot;Optimization Finished!&quot;) 8.7 Evaluate the Multi-Layer Perceptron Model We restore the best model saved from the training phase and run this model on the test set to get the testing accuracy. We also restore the weights associated with the best model and then use them to calculate the importance score for each feature and print the scores out in the descending order. We plot the feature importance and save the plot permanently to S3 using the “smart_open” Python library. The following code covers the steps 108-110. with tf.compat.v1.Session() as sess: saver.restore(sess, checkpoint1) weights_saver.restore(sess, checkpoint2) graph = tf.compat.v1.get_default_graph() training_accuracy = sess.run(accuracy, feed_dict = {x: x_train, y: y_train, rate: 1 - training_dropout}) validation_accuracy = sess.run(accuracy, feed_dict = {x: x_validation, y: y_validation, rate: 0}) print(&quot;Results using the best Validation_Accuracy:&quot;) print(&quot;Training Accuracy =&quot;, training_accuracy) print(&quot;Validation Accuracy =&quot;, validation_accuracy) testing_prediction, testing_accuracy = sess.run([pred, accuracy], feed_dict = {x: x_test, y: y_test, rate: 0}) print() print(&quot;Results using the best Validation_Accuracy:&quot;) print(&quot;Testing Accuracy =&quot;, testing_accuracy) w1 = w1.eval(session=sess) df = pd.DataFrame(w1) print(df.shape) print(df.head(5)) df.loc[:, &quot;sum&quot;] = df.sum(axis = 1) print(df.head(5)) dt_importances = abs(df.loc[:, &quot;sum&quot;].values) dt_indices = np.argsort(dt_importances)[:: -1] print(&quot;Feature ranking:: ANN:&quot;) for f in range(x_train.shape[1]): print(&quot;%d. feature %d importance (%f)&quot; % (f + 1, dt_indices[f], dt_importances[dt_indices[f]])) plt.figure() plt.title(&quot;Feature importances:: ANN&quot; ) plt.bar(range(x_train.shape[1]), dt_importances[dt_indices], color = &quot;r&quot;, align = &quot;center&quot;) plt.xticks(range(x_train.shape[1]), dt_indices) plt.xlim([-1, x_train.shape[1]]) plt.savefig(&#39;./importance.png&#39;) importance_key = &#39;feature_importance.png&#39; imp_graph_path = &#39;s3://{}:{}@{}/{}&#39;.format(aws_key, aws_secret, bucket_name, importance_key) with open(&#39;./importance.png&#39;, &#39;rb&#39;) as f: content = f.read() with open(imp_graph_path, &#39;wb&#39;) as fout: fout.write(content) Finally, we also plot the training vs. validation accuracy and the training vs. validation cost over the entire training epochs and save the plot permanently to S3 using the “smart_open” Python library. # Plot accuracy and cost summaries f, (ax1, ax2) = plt.subplots(2, 1, sharex = True, figsize = (10,4)) ax1.plot(train_accuracy_summary) # blue ax1.plot(valid_accuracy_summary) # green ax1.set_title(&#39;Accuracy&#39;) ax2.plot(train_cost_summary) # blue ax2.plot(valid_cost_summary) # green ax2.set_title(&#39;Cost&#39;) plt.xlabel(&#39;Epochs&#39;) plt.savefig(&#39;./epochs.png&#39;) epoch_key = &#39;training_epoch.png&#39; epoch_graph_path = &#39;s3://{}:{}@{}/{}&#39;.format(aws_key, aws_secret, bucket_name, epoch_key) with open(&#39;./epochs.png&#39;, &#39;rb&#39;) as f: content = f.read() with smart_open(epoch_graph_path, &#39;wb&#39;) as fout: fout.write(content) "],
["get-the-results-locally.html", "9 Get the Results Locally", " 9 Get the Results Locally You could copy the results printed out in the Output cell in the Jupyter notebook and paste it to an editor locally. To download files or folders in S3 bucket to local, open the Amazon S3 console. In the Bucket name list, choose the name of the bucket that you want to download an object from. In the Name list, select the check box next to the object you want to download, and then choose Download on the object description page that appears. Congratulations! You are done. To clean up your environment, do the following: To sign out of the Jupyter notebook, click Logout. To clean up the S3 bucket objects after downloading, open the Amazon S3 console. In the Bucket name list, choose the name of the bucket that you want to delete an object from. In the Name list, select the check box next to the objects and folders that you want to delete, choose Delete. If you don’t need the instance anymore, open the Amazon EC2 console Instances page. In the Name list, select the instance that you want to do the operation. Right click on the instance. In the Instance State drop-down menu, either stop or terminate the instance. The instance state status will change in a few minutes. To sign out of AWS Management Console, click on your username at the top of the console, and then click Sign Out. "]
]
